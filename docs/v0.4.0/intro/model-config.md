# æ¨¡å‹é…ç½®

## å¯¹è¯æ¨¡å‹

ç³»ç»Ÿæ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å•†ï¼Œé€šè¿‡é…ç½®å¯¹åº”çš„ API å¯†é’¥å³å¯ä½¿ç”¨ï¼š

| æœåŠ¡å•†                                           | ç¯å¢ƒå˜é‡                | ç‰¹ç‚¹                  |
| ------------------------------------------------ | ----------------------- | --------------------- |
| [ç¡…åŸºæµåŠ¨](https://cloud.siliconflow.cn/i/Eo5yTHGJ) | `SILICONFLOW_API_KEY` | ğŸ†“ å…è´¹é¢åº¦ï¼Œé»˜è®¤æ¨è |
| OpenAI                                           | `OPENAI_API_KEY`      | GPT ç³»åˆ—æ¨¡å‹          |
| DeepSeek                                         | `DEEPSEEK_API_KEY`    | å›½äº§å¤§æ¨¡å‹            |
| OpenRouter                                       | `OPENROUTER_API_KEY`  | å¤šæ¨¡å‹èšåˆå¹³å°        |
| æ™ºè°±æ¸…è¨€                                         | `ZHIPUAI_API_KEY`     | GLM ç³»åˆ—æ¨¡å‹          |
| é˜¿é‡Œäº‘ç™¾ç‚¼                                       | `DASHSCOPE_API_KEY`   | é€šä¹‰åƒé—®ç³»åˆ—          |

å…¶ä½™è¿˜æ”¯æŒç«å±±è±†åŒ…ã€Togetherã€vLLMã€Ollama ç­‰ã€‚

### é…ç½®æ–¹æ³•

åœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ å¯¹åº”çš„ç¯å¢ƒå˜é‡ï¼š



::: tip å…è´¹è·å– API Key
[ç¡…åŸºæµåŠ¨](https://cloud.siliconflow.cn/i/Eo5yTHGJ) æ³¨å†Œå³é€ 14 å…ƒé¢åº¦ï¼Œæ”¯æŒå¤šç§å¼€æºæ¨¡å‹ã€‚
:::

<<< @/../.env.template#model_provider{bash 5}

### é»˜è®¤å¯¹è¯æ¨¡å‹æ ¼å¼

ç³»ç»Ÿçš„é»˜è®¤å¯¹è¯æ¨¡å‹å¯ä»¥åœ¨è®¾ç½®é¡µé¢é…ç½®ï¼Œä¹Ÿå¯ä»¥é€šè¿‡é…ç½®é¡¹ `default_model` æŒ‡å®šï¼Œæ ¼å¼ç»Ÿä¸€ä¸º `æ¨¡å‹æä¾›å•†/æ¨¡å‹åç§°`ï¼Œä¾‹å¦‚ï¼š

```yaml
default_model: siliconflow/deepseek-ai/DeepSeek-V3.2
```

## è‡ªå®šä¹‰æ¨¡å‹ä¾›åº”å•†

::: tip é…ç½®ç³»ç»Ÿå‡çº§ (v0.3.x)
ä» `v0.3.x` ç‰ˆæœ¬å¼€å§‹ï¼Œæ¨¡å‹é…ç½®ç³»ç»Ÿå·²å‡çº§ä¸ºåŸºäº Pydantic BaseModel çš„ç±»å‹å®‰å…¨é…ç½®ï¼Œæ”¯æŒ TOML æ ¼å¼çš„ç”¨æˆ·é…ç½®æ–‡ä»¶ã€‚

- **é»˜è®¤é…ç½®**: `src/config/static/models.py` (Python ä»£ç )
- **ç”¨æˆ·é…ç½®**: `saves/config/base.toml` (TOML æ ¼å¼ï¼Œä»…ä¿å­˜ç”¨æˆ·ä¿®æ”¹)
- **è‡ªå®šä¹‰ä¾›åº”å•†**: `saves/config/custom_providers.toml` (ç‹¬ç«‹é…ç½®æ–‡ä»¶)
  :::

ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„è‡ªå®šä¹‰ä¾›åº”å•†ç®¡ç†åŠŸèƒ½ï¼Œæ”¯æŒé€šè¿‡ Web ç•Œé¢ç›´æ¥æ·»åŠ ã€ç¼–è¾‘ã€æµ‹è¯•å’Œåˆ é™¤è‡ªå®šä¹‰æ¨¡å‹ä¾›åº”å•†ã€‚

### ä½¿ç”¨æ–¹æ³•

ç³»ç»Ÿæ”¯æŒä»»ä½• OpenAI å…¼å®¹çš„äº‘æœåŠ¡æä¾›å•†

#### 1. Web ç•Œé¢æ“ä½œï¼ˆæ¨èï¼‰

è®¿é—® **ç³»ç»Ÿè®¾ç½® > æ¨¡å‹é…ç½®**ï¼Œåœ¨"è‡ªå®šä¹‰ä¾›åº”å•†"éƒ¨åˆ†ç‚¹å‡» **æ·»åŠ è‡ªå®šä¹‰ä¾›åº”å•†**ã€‚è¿™é‡Œçš„å¯†é’¥å¯ä»¥ç›´æ¥å¡«å†™ä¹Ÿå¯ä»¥å¡«å†™å¯¹åº”çš„ç¯å¢ƒå˜é‡åç§°ã€‚

#### 2. é…ç½®æ–‡ä»¶æ“ä½œ

å¦‚éœ€é€šè¿‡é…ç½®æ–‡ä»¶ç®¡ç†ï¼Œç¼–è¾‘ `saves/config/custom_providers.toml`ï¼š

```toml
[model_names.local-vllm]
name = "æœ¬åœ° vLLM æœåŠ¡"
url = "https://docs.vllm.ai"
base_url = "http://localhost:8000/v1"
default = "Qwen/Qwen2.5-7B-Instruct"
env = "LOCAL_VLLM_API_KEY"
models = [
    "Qwen/Qwen2.5-7B-Instruct",
    "Qwen/Qwen2.5-14B-Instruct",
]
custom = true

[model_names.local-ollama]
name = "æœ¬åœ° Ollama"
url = "https://ollama.com"
base_url = "http://localhost:11434/v1"
default = "llama3.2"
env = "NO_API_KEY"
models = ["llama3.2", "qwen2.5"]
custom = true
```

ç„¶ååœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ å¯¹åº”çš„ç¯å¢ƒå˜é‡ï¼š

```env
LOCAL_VLLM_API_KEY=your_api_key_here
```

### API ç«¯ç‚¹

ç³»ç»Ÿæä¾›ä»¥ä¸‹ API ç«¯ç‚¹ç®¡ç†è‡ªå®šä¹‰ä¾›åº”å•†ï¼š

- `GET /api/system/custom-providers` - è·å–æ‰€æœ‰è‡ªå®šä¹‰ä¾›åº”å•†
- `POST /api/system/custom-providers` - æ·»åŠ è‡ªå®šä¹‰ä¾›åº”å•†
- `PUT /api/system/custom-providers/{provider_id}` - æ›´æ–°è‡ªå®šä¹‰ä¾›åº”å•†
- `DELETE /api/system/custom-providers/{provider_id}` - åˆ é™¤è‡ªå®šä¹‰ä¾›åº”å•†
- `POST /api/system/custom-providers/{provider_id}/test` - æµ‹è¯•ä¾›åº”å•†è¿æ¥

### å¸¸è§é…ç½®ç¤ºä¾‹

#### vLLM æœ¬åœ°æœåŠ¡

```toml
[model_names.vllm-local]
name = "vLLM æœ¬åœ°æœåŠ¡"
base_url = "http://localhost:8000/v1"
default = "Qwen/Qwen2.5-7B-Instruct"
env = "NO_API_KEY"
models = [
    "Qwen/Qwen2.5-7B-Instruct",
    "Qwen/Qwen2.5-14B-Instruct",
    "meta-llama/Llama-3.1-8B-Instruct"
]
```

#### Ollama æœ¬åœ°æœåŠ¡

```toml
[model_names.ollama-local]
name = "Ollama æœ¬åœ°æœåŠ¡"
base_url = "http://localhost:11434/v1"
default = "llama3.2"
env = "NO_API_KEY"
models = [
    "llama3.2:latest",
    "qwen2.5:latest",
    "codellama:latest"
]
```

#### ç¬¬ä¸‰æ–¹ API ä¸­è½¬æœåŠ¡

```toml
[model_names.api-proxy]
name = "API ä¸­è½¬æœåŠ¡"
base_url = "https://api-proxy.example.com/v1"
default = "gpt-3.5-turbo"
env = "API_PROXY_KEY"
models = [
    "gpt-3.5-turbo",
    "gpt-4",
    "claude-3-sonnet"
]
```

### æ•…éšœæ’é™¤

1. **æµ‹è¯•è¿æ¥å¤±è´¥**: æ£€æŸ¥ API åœ°å€æ ¼å¼å’Œ API å¯†é’¥é…ç½®
2. **æ¨¡å‹ä¸å¯ç”¨**: ç¡®è®¤æ¨¡å‹åç§°æ‹¼å†™å’ŒæœåŠ¡ç«¯æ˜¯å¦æ”¯æŒè¯¥æ¨¡å‹
3. **æƒé™é”™è¯¯**: ç¡®ä¿ç”¨æˆ·å…·æœ‰ç®¡ç†å‘˜æƒé™
4. **é…ç½®æœªç”Ÿæ•ˆ**: æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®å’ŒæœåŠ¡é‡å¯çŠ¶æ€

## åµŒå…¥æ¨¡å‹å’Œé‡æ’åºæ¨¡å‹

#### 1. é…ç½®æ¨¡å‹ä¿¡æ¯

åœ¨ `src/config/static/models.py` ä¸­çš„é»˜è®¤é…ç½®éƒ¨åˆ†æ·»åŠ ï¼š

```python
# é»˜è®¤åµŒå…¥æ¨¡å‹é…ç½®
DEFAULT_EMBED_MODELS: dict[str, EmbedModelInfo] = {
    # ... ç°æœ‰é…ç½® ...

    "vllm/Qwen/Qwen3-Embedding-0.6B": EmbedModelInfo(
        name="Qwen/Qwen3-Embedding-0.6B",
        dimension=1024,
        base_url="http://localhost:8000/v1/embeddings",
        api_key="no_api_key",
    ),
}

# é»˜è®¤é‡æ’åºæ¨¡å‹é…ç½®
DEFAULT_RERANKERS: dict[str, RerankerInfo] = {
    # ... ç°æœ‰é…ç½® ...

    "vllm/BAAI/bge-reranker-v2-m3": RerankerInfo(
        name="BAAI/bge-reranker-v2-m3",
        base_url="http://localhost:8000/v1/rerank",
        api_key="no_api_key",
    ),
}
```

#### 2. åŠ¨æ€é…ç½®ï¼ˆå¯é€‰ï¼‰

ä½ ä¹Ÿå¯ä»¥é€šè¿‡ä»£ç åŠ¨æ€æ·»åŠ æœ¬åœ°æ¨¡å‹ï¼š

```python
from src.config import config
from src.config.static.models import EmbedModelInfo, RerankerInfo

# æ·»åŠ æœ¬åœ°åµŒå…¥æ¨¡å‹
config.embed_model_names["local/embed-model"] = EmbedModelInfo(
    name="local-embed-model",
    dimension=1024,
    base_url="http://localhost:8000/v1/embeddings",
    api_key="no_api_key",
)

# æ·»åŠ æœ¬åœ°é‡æ’åºæ¨¡å‹
config.reranker_names["local/reranker-model"] = RerankerInfo(
    name="local-reranker-model",
    base_url="http://localhost:8000/v1/rerank",
    api_key="no_api_key",
)

# ä¿å­˜é…ç½®
config.save()
```

#### 3. å¯åŠ¨æ¨¡å‹æœåŠ¡

```bash
# å¯åŠ¨åµŒå…¥æ¨¡å‹
vllm serve Qwen/Qwen3-Embedding-0.6B \
  --task embed \
  --dtype auto \
  --port 8000

# å¯åŠ¨é‡æ’åºæ¨¡å‹
vllm serve BAAI/bge-reranker-v2-m3 \
  --task score \
  --dtype fp16 \
  --port 8000
```
